{
  "d_model": 128,
  "n_heads": 8,
  "num_layers": 3,
  "dim_feedforward": 256,
  "dropout": 0.1,
  "pos_encoding": "fixed",
  "activation": "gelu",
  "norm": "BatchNorm"
}